
@article{allen2022evaluating,
  title={Evaluating forecasts for high-impact events using transformed kernel scores},
  author={Allen, Sam and Ginsbourger, David and Ziegel, Johanna},
  journal={arXiv preprint arXiv:2202.12732},
  year={2022}
}

@article{bracher2021evaluating,
  title={Evaluating epidemic forecasts in an interval format},
  author={Bracher, Johannes and Ray, Evan L and Gneiting, Tilmann and Reich, Nicholas G},
  journal={PLoS computational biology},
  volume={17},
  number={2},
  pages={e1008618},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	url = {https://doi.org/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	year = {2007},
}

@article{holzmann2017focusing,
	author = {Holzmann, Hajo and Klar, Bernhard},
	journal = {The Annals of Applied Statistics},
	pages = {2404--2431},
	publisher = {Institute of Mathematical Statistics},
	title = {Focusing on regions of interest in forecast evaluation},
	volume = {11},
	year = {2017}
}

@phdthesis{jordan_facets_2016,
    author       = {Jordan, Alexander},
    year         = {2016},
    title        = {Facets of forecast evaluation},
    doi          = {10.5445/IR/1000063629},
    publisher    = {{Karlsruher Institut f{\"{u}}r Technologie (KIT)}},
    keywords     = {Mixture representations, Murphy diagrams, ranking stability, CRPS},
    pagetotal    = {112},
    school       = {Karlsruher Institut für Technologie (KIT)},
    language     = {english}
}

@article{scheuerer_variogram-based_2015,
	title = {Variogram-{Based} {Proper} {Scoring} {Rules} for {Probabilistic} {Forecasts} of {Multivariate} {Quantities}},
	url = {https://journals.ametsoc.org/view/journals/mwre/143/4/mwr-d-14-00269.1.xml},
	doi = {10.1175/MWR-D-14-00269.1},
	abstract = {Abstract Proper scoring rules provide a theoretically principled framework for the quantitative assessment of the predictive performance of probabilistic forecasts. While a wide selection of such scoring rules for univariate quantities exists, there are only few scoring rules for multivariate quantities, and many of them require that forecasts are given in the form of a probability density function. The energy score, a multivariate generalization of the continuous ranked probability score, is the only commonly used score that is applicable in the important case of ensemble forecasts, where the multivariate predictive distribution is represented by a finite sample. Unfortunately, its ability to detect incorrectly specified correlations between the components of the multivariate quantity is somewhat limited. In this paper the authors present an alternative class of proper scoring rules based on the geostatistical concept of variograms. The sensitivity of these variogram-based scoring rules to incorrectly predicted means, variances, and correlations is studied in a number of examples with simulated observations and forecasts; they are shown to be distinctly more discriminative with respect to the correlation structure. This conclusion is confirmed in a case study with postprocessed wind speed forecasts at five wind park locations in Colorado.},
	journal = {Monthly Weather Review},
	author = {Scheuerer, Michael and Hamill, Thomas M.},
	year = {2015},
}

@article{taillardat_calibrated_2016,
	title = {Calibrated {Ensemble} {Forecasts} {Using} {Quantile} {Regression} {Forests} and {Ensemble} {Model} {Output} {Statistics}},
	url = {http://journals.ametsoc.org/doi/10.1175/MWR-D-15-0260.1},
	doi = {10.1175/MWR-D-15-0260.1},
	abstract = {Abstract
            Ensembles used for probabilistic weather forecasting tend to be biased and underdispersive. This paper proposes a statistical method for postprocessing ensembles based on quantile regression forests (QRF), a generalization of random forests for quantile regression. This method does not fit a parametric probability density function (PDF) like in ensemble model output statistics (EMOS) but provides an estimation of desired quantiles. This is a nonparametric approach that eliminates any assumption on the variable subject to calibration. This method can estimate quantiles using not only members of the ensemble but any predictor available including statistics on other variables.
            The method is applied to the Météo-France 35-member ensemble forecast (PEARP) for surface temperature and wind speed for available lead times from 3 up to 54 h and compared to EMOS. All postprocessed ensembles are much better calibrated than the PEARP raw ensemble and experiments on real data also show that QRF performs better than EMOS, and can bring a real gain for human forecasters compared to EMOS. QRF provides sharp and reliable probabilistic forecasts. At last, classical scoring rules to verify predictive forecasts are completed by the introduction of entropy as a general measure of reliability.},
	journal = {Monthly Weather Review},
	author = {Taillardat, Maxime and Mestre, Olivier and Zamo, Michaël and Naveau, Philippe},
	year = {2016},
}

@article{zamo_estimation_2018,
	title = {Estimation of the {Continuous} {Ranked} {Probability} {Score} with {Limited} {Information} and {Applications} to {Ensemble} {Weather} {Forecasts}},
	url = {https://doi.org/10.1007/s11004-017-9709-7},
	doi = {10.1007/s11004-017-9709-7},
	abstract = {The continuous ranked probability score (CRPS) is a much used measure of performance for probabilistic forecasts of a scalar observation. It is a quadratic measure of the difference between the forecast cumulative distribution function (CDF) and the empirical CDF of the observation. Analytic formulations of the CRPS can be derived for most classical parametric distributions, and be used to assess the efficiency of different CRPS estimators. When the true forecast CDF is not fully known, but represented as an ensemble of values, the CRPS is estimated with some error. Thus, using the CRPS to compare parametric probabilistic forecasts with ensemble forecasts may be misleading due to the unknown error of the estimated CRPS for the ensemble. With simulated data, the impact of the type of the verified ensemble (a random sample or a set of quantiles) on the CRPS estimation is studied. Based on these simulations, recommendations are issued to choose the most accurate CRPS estimator according to the type of ensemble. The interest of these recommendations is illustrated with real ensemble weather forecasts. Also, relationships between several estimators of the CRPS are demonstrated and used to explain the differences of accuracy between the estimators.},
	journal = {Mathematical Geosciences},
	author = {Zamo, Michaël and Naveau, Philippe},
	year = {2018},
}

@article{winkler1972decision,
  title={A decision-theoretic approach to interval estimation},
  author={Winkler, Robert L},
  journal={Journal of the American Statistical Association},
  volume={67},
  number={337},
  pages={187--191},
  year={1972},
  publisher={Taylor \& Francis}
}

@article{ziel2019multivariate,
  title={Multivariate forecasting evaluation: On sensitive and strictly proper scoring rules},
  author={Ziel, Florian and Berk, Kevin},
  journal={arXiv preprint arXiv:1910.07325},
  year={2019}
}
